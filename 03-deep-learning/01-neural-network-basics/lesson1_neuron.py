# -*- coding: utf-8 -*-
"""
第1课：神经网络基础 - 理解神经元

神经元是神经网络最基本的单元，就像大脑中的一个"决策点"
"""

import numpy as np

print("=" * 60)
print("第1课：理解神经元")
print("=" * 60)

# ============================================================
# 1. 最简单的例子：判断今天要不要带伞
# ============================================================
print("\n【例子1】判断今天要不要带伞")
print("-" * 40)

# 输入信息（特征）
天气预报说下雨 = 1      # 1=是, 0=否
天上有乌云 = 1          # 1=是, 0=否
地上是湿的 = 0          # 1=是, 0=否

# 每个信息的重要程度（权重）
# 权重越大，这个因素越重要
天气预报权重 = 0.6      # 天气预报比较准，权重高
乌云权重 = 0.3          # 乌云也是个信号，但不一定下雨
地面权重 = 0.1          # 地上湿可能是洒水车

# 神经元的计算：加权求和
总分 = (天气预报说下雨 * 天气预报权重 +
       天上有乌云 * 乌云权重 +
       地上是湿的 * 地面权重)

print(f"天气预报说下雨: {天气预报说下雨} × 权重 {天气预报权重} = {天气预报说下雨 * 天气预报权重}")
print(f"天上有乌云:     {天上有乌云} × 权重 {乌云权重} = {天上有乌云 * 乌云权重}")
print(f"地上是湿的:     {地上是湿的} × 权重 {地面权重} = {地上是湿的 * 地面权重}")
print(f"总分 = {总分}")

# 阈值判断：分数超过0.5就带伞
阈值 = 0.5
决定 = "带伞" if 总分 > 阈值 else "不带伞"
print(f"\n阈值 = {阈值}, 总分 {总分} {'>' if 总分 > 阈值 else '<='} 阈值")
print(f"决定: {决定}")


# ============================================================
# 2. 用代码写成神经元
# ============================================================
print("\n\n【用代码表示神经元】")
print("-" * 40)

def 简单神经元(输入, 权重, 阈值=0.5):
    """
    最简单的神经元

    参数:
        输入: 一组输入值 [x1, x2, x3, ...]
        权重: 每个输入的重要程度 [w1, w2, w3, ...]
        阈值: 超过这个值就"激活"

    返回:
        1 (激活) 或 0 (不激活)
    """
    # 加权求和
    总分 = sum(x * w for x, w in zip(输入, 权重))

    # 阈值判断（这就是最简单的"激活函数"）
    return 1 if 总分 > 阈值 else 0

# 测试
输入 = [1, 1, 0]  # 天气预报说下雨，有乌云，地不湿
权重 = [0.6, 0.3, 0.1]

结果 = 简单神经元(输入, 权重)
print(f"输入: {输入}")
print(f"权重: {权重}")
print(f"输出: {结果} ({'带伞' if 结果 else '不带伞'})")


# ============================================================
# 3. 激活函数：让输出更"圆滑"
# ============================================================
print("\n\n【激活函数】")
print("-" * 40)

print("""
问题：简单的0/1输出太"生硬"
- 总分0.49 → 不带伞
- 总分0.51 → 带伞
差一点点，结果完全不同！

解决：使用"激活函数"让输出更圆滑
""")

def sigmoid(x):
    """
    Sigmoid 激活函数

    把任何数字压缩到 0~1 之间：
    - 很小的负数 → 接近 0
    - 0 → 0.5
    - 很大的正数 → 接近 1

    就像一个"软化"的开关
    """
    return 1 / (1 + np.exp(-x))

# 演示 sigmoid 的效果
print("Sigmoid 函数的输出：")
for x in [-5, -2, -1, 0, 1, 2, 5]:
    print(f"  sigmoid({x:2d}) = {sigmoid(x):.4f}")

print("""
可以看到：
- 负数 → 趋向于 0
- 正数 → 趋向于 1
- 中间值 → 平滑过渡
""")


# ============================================================
# 4. 完整的神经元
# ============================================================
print("\n\n【完整的神经元】")
print("-" * 40)

def 神经元(输入, 权重, 偏置=0):
    """
    完整的神经元

    计算过程：
    1. 加权求和: z = w1*x1 + w2*x2 + ... + b
    2. 激活函数: output = sigmoid(z)

    偏置(bias)：类似于"基础分"
    - 即使所有输入都是0，也可以有输出
    - 让神经元更灵活
    """
    # 加权求和 + 偏置
    z = sum(x * w for x, w in zip(输入, 权重)) + 偏置

    # 通过激活函数
    输出 = sigmoid(z)

    return 输出

# 测试完整的神经元
输入 = [1, 1, 0]
权重 = [0.6, 0.3, 0.1]
偏置 = -0.5  # 负偏置意味着"默认倾向于不带伞"

输出 = 神经元(输入, 权重, 偏置)
print(f"输入: {输入}")
print(f"权重: {权重}")
print(f"偏置: {偏置}")
print(f"输出: {输出:.4f}")
print(f"解读: {输出:.1%} 的概率需要带伞")


# ============================================================
# 5. 动手实验：改变权重看效果
# ============================================================
print("\n\n【动手实验】改变权重看效果")
print("-" * 40)

def 测试场景(场景名, 天气预报, 有乌云, 地湿):
    输入 = [天气预报, 有乌云, 地湿]
    权重 = [0.8, 0.4, 0.2]  # 调整后的权重
    偏置 = -0.5

    输出 = 神经元(输入, 权重, 偏置)
    print(f"{场景名}: 输入{输入} → {输出:.1%} 带伞概率")

print("不同场景的带伞概率：")
测试场景("晴天无云",     0, 0, 0)
测试场景("有点乌云",     0, 1, 0)
测试场景("预报有雨",     1, 0, 0)
测试场景("预报+乌云",    1, 1, 0)
测试场景("全部信号",     1, 1, 1)


# ============================================================
# 总结
# ============================================================
print("\n\n" + "=" * 60)
print("第1课总结")
print("=" * 60)
print("""
1. 神经元 = 加权求和 + 激活函数

2. 公式：output = activation(w1*x1 + w2*x2 + ... + bias)

3. 权重(weight)：决定每个输入的重要程度
   - 权重大 → 这个输入很重要
   - 权重小 → 这个输入不太重要
   - 权重为负 → 这个输入起反作用

4. 偏置(bias)：类似"基础分"，让神经元更灵活

5. 激活函数：让输出更"圆滑"
   - Sigmoid: 输出 0~1，像"概率"
   - 还有 ReLU, Tanh 等（下节课讲）

下一课预告：多个神经元组成"层"，多层组成"网络"
""")
