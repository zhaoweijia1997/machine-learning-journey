# 第4课学习指南：反向传播与梯度下降

> 这是深度学习最核心的一课——让网络自己学习！

---

## 核心概念速览

| 概念 | 一句话解释 |
|------|-----------|
| 梯度 | 损失对权重的变化率，告诉我们"往哪调、调多少" |
| 梯度下降 | 沿着梯度的反方向更新权重 |
| 反向传播 | 用链式法则从后往前计算每层的梯度 |
| 学习率 | 每一步更新的大小 |

---

## 常见问题解答

### Q1: 什么是梯度？

**梯度 = 损失函数对权重的偏导数**

直觉理解：
- 你站在山坡上，梯度就是"最陡的方向"
- 梯度的符号告诉你"往左还是往右"
- 梯度的大小告诉你"坡有多陡"

```
例子：
    损失 = (预测 - 真实)²
    梯度 = 2 × (预测 - 真实) × 输入

    如果预测太小，梯度是负的 → 权重要变大
    如果预测太大，梯度是正的 → 权重要变小
```

---

### Q2: 为什么是"减"梯度？

```
新权重 = 旧权重 - 学习率 × 梯度
                 ↑
              为什么是减？
```

**因为梯度指向损失增大的方向！**

- 梯度 > 0：往这个方向走，损失会增大
- 我们要让损失减小
- 所以要往反方向走 → 减去梯度

这就像下山：
- 梯度指向"上坡"方向
- 我们要"下坡"
- 所以反着走

---

### Q3: 学习率怎么选？

学习率控制"每一步走多大"。

| 学习率 | 效果 |
|--------|------|
| 太小 (0.001) | 收敛很慢，但稳定 |
| 适中 (0.01-0.1) | 平衡速度和稳定性 |
| 太大 (>0.5) | 可能震荡甚至发散！ |

**建议**：从 0.01 或 0.1 开始，观察损失变化来调整。

---

### Q4: 什么是反向传播？

对于多层网络，需要计算每一层的梯度。

**问题**：隐藏层的权重离输出很远，怎么算梯度？

**答案**：链式法则！

```
d(损失)/d(隐藏层权重)
    = d(损失)/d(输出)
    × d(输出)/d(隐藏层输出)
    × d(隐藏层输出)/d(权重)
```

**反向传播** = 从输出层开始，一层层往回算梯度

```
前向传播: 输入 → 隐藏层 → 输出 → 损失
反向传播: 输入 ← 隐藏层 ← 输出 ← 损失
                        梯度传递方向
```

---

### Q5: 训练循环是怎样的？

```python
for epoch in range(1000):
    # 1. 前向传播：算预测
    prediction = network(input)

    # 2. 计算损失
    loss = (prediction - label) ** 2

    # 3. 反向传播：算梯度
    gradients = backpropagate(loss)

    # 4. 更新权重
    for w, g in zip(weights, gradients):
        w = w - learning_rate * g
```

重复这个循环，损失会越来越小，网络越来越准！

---

## 完整的知识体系

恭喜！学完第4课，你已经掌握了神经网络的核心原理：

```
第1课：神经元      → 计算单元
第2课：多层网络    → 网络结构
第3课：损失函数    → 评估好坏
第4课：反向传播    → 自动学习  ← 你在这里！
```

**你现在理解的内容**：
- [x] 神经网络是什么
- [x] 数据如何流过网络
- [x] 如何衡量预测好坏
- [x] 如何自动调整权重

---

## 动手实验建议

1. **打开交互网页**，点击"单步"看梯度下降过程
2. **调整学习率**，观察收敛速度的变化
3. **试试极端情况**：学习率 0.3 会发生什么？
4. **运行 Python 代码**，看 XOR 问题如何被解决

---

## 关键公式

```python
# 梯度下降
gradient = 2 * (prediction - label) * input
new_weight = old_weight - learning_rate * gradient

# 反向传播（链式法则）
d_loss_d_w = d_loss_d_output * d_output_d_hidden * d_hidden_d_w
```

---

## 学习检查

完成本课后，你应该能回答：

- [ ] 梯度是什么？有什么用？
- [ ] 为什么要"减去"梯度？
- [ ] 学习率太大/太小会怎样？
- [ ] 反向传播解决什么问题？
- [ ] 训练循环包含哪几步？

---

## 下一步学习

神经网络基础已经学完！接下来可以学习：

- **第5-7课**：卷积神经网络 (CNN) - 图像识别的利器
- **第8-10课**：人脸识别专题 - 理解你之前用的模型

或者直接动手：用 PyTorch 训练一个真正的模型！

---

<p align="center"><b>🎉 恭喜完成神经网络基础！你已经理解了深度学习的核心原理！</b></p>
