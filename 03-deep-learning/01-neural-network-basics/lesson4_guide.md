# 第4课学习指南：反向传播与梯度下降

> 这是深度学习最核心的一课——让网络自己学习！

---

## 核心概念速览

| 概念 | 一句话解释 |
|------|-----------|
| 梯度 | 损失对权重的变化率，告诉我们"往哪调、调多少" |
| 梯度下降 | 沿着梯度的反方向更新权重 |
| 反向传播 | 用链式法则从后往前计算每层的梯度 |
| 学习率 | 每一步更新的大小 |

---

## 常见问题解答

### Q1: 什么是梯度？

**梯度 = 损失函数对权重的偏导数**

直觉理解：
- 你站在山坡上，梯度就是"最陡的方向"
- 梯度的符号告诉你"往左还是往右"
- 梯度的大小告诉你"坡有多陡"

```
例子：
    损失 = (预测 - 真实)²
    梯度 = 2 × (预测 - 真实) × 输入

    如果预测太小，梯度是负的 → 权重要变大
    如果预测太大，梯度是正的 → 权重要变小
```

---

### Q2: 为什么是"减"梯度？

```
新权重 = 旧权重 - 学习率 × 梯度
                 ↑
              为什么是减？
```

**因为梯度指向损失增大的方向！**

- 梯度 > 0：往这个方向走，损失会增大
- 我们要让损失减小
- 所以要往反方向走 → 减去梯度

这就像下山：
- 梯度指向"上坡"方向
- 我们要"下坡"
- 所以反着走

---

### Q3: 学习率怎么选？

学习率控制"每一步走多大"。

| 学习率 | 效果 |
|--------|------|
| 太小 (0.001) | 收敛很慢，但稳定 |
| 适中 (0.01-0.1) | 平衡速度和稳定性 |
| 太大 (>0.5) | 可能震荡甚至发散！ |

**建议**：从 0.01 或 0.1 开始，观察损失变化来调整。

---

### Q4: 什么是反向传播？

对于多层网络，需要计算每一层的梯度。

**问题**：隐藏层的权重离输出很远，怎么算梯度？

**答案**：链式法则！

```
d(损失)/d(隐藏层权重)
    = d(损失)/d(输出)
    × d(输出)/d(隐藏层输出)
    × d(隐藏层输出)/d(权重)
```

**反向传播** = 从输出层开始，一层层往回算梯度

```
前向传播: 输入 → 隐藏层 → 输出 → 损失
反向传播: 输入 ← 隐藏层 ← 输出 ← 损失
                        梯度传递方向
```

---

### Q5: 训练循环是怎样的？

```python
for epoch in range(1000):
    # 1. 前向传播：算预测
    prediction = network(input)

    # 2. 计算损失
    loss = (prediction - label) ** 2

    # 3. 反向传播：算梯度
    gradients = backpropagate(loss)

    # 4. 更新权重
    for w, g in zip(weights, gradients):
        w = w - learning_rate * g
```

重复这个循环，损失会越来越小，网络越来越准！

---

## 完整的知识体系

恭喜！学完第4课，你已经掌握了神经网络的核心原理：

```
第1课：神经元      → 计算单元
第2课：多层网络    → 网络结构
第3课：损失函数    → 评估好坏
第4课：反向传播    → 自动学习  ← 你在这里！
```

**你现在理解的内容**：
- [x] 神经网络是什么
- [x] 数据如何流过网络
- [x] 如何衡量预测好坏
- [x] 如何自动调整权重

---

## 动手实验建议

1. **打开交互网页**，点击"单步"看梯度下降过程
2. **调整学习率**，观察收敛速度的变化
3. **试试极端情况**：学习率 0.3 会发生什么？
4. **运行 Python 代码**，看 XOR 问题如何被解决

---

## 关键公式

```python
# 梯度下降
gradient = 2 * (prediction - label) * input
new_weight = old_weight - learning_rate * gradient

# 反向传播（链式法则）
d_loss_d_w = d_loss_d_output * d_output_d_hidden * d_hidden_d_w
```

---

## 学习检查

完成本课后，你应该能回答：

- [ ] 梯度是什么？有什么用？
- [ ] 为什么要"减去"梯度？
- [ ] 学习率太大/太小会怎样？
- [ ] 反向传播解决什么问题？
- [ ] 训练循环包含哪几步？

---

## 深入理解：梯度下降的本质

### 为什么不能遍历所有权重？

直觉上，找最优权重可以"试遍所有值"。但这不可行：

```
问题1：权重是连续的
    w 可以是 0.1, 0.11, 0.111...
    有无穷多个值！

问题2：参数数量巨大
    小网络：几百个参数
    GPT：上千亿个参数
    100个参数各试100个值 → 100^100 种组合
```

所以需要更聪明的方法：**看坡度（梯度）来决定方向**。

---

### 损失函数为什么能用梯度下降？

你可能会问：`y = x × w` 对 w 求导不就是常数 x 吗？

**关键**：我们求导的是**损失函数**，不是预测函数！

```
预测函数: y = x × w           → 一阶，导数是常数
损失函数: L = (x×w - y_true)² → 二次，导数包含 w
```

损失函数展开：`L = x²w² - 2xy_true·w + y_true²`

这是关于 w 的**抛物线**，有最低点，所以梯度下降有效！

---

### 学习率到底是什么？

**学习率 ≠ 时间频率**，而是**步长**。

```
新权重 = 旧权重 - 学习率 × 梯度
                    ↑
                每步走多大
```

| 概念 | 含义 |
|------|------|
| 循环次数 | 走多少步 |
| 学习率 | 每步走多大 |

---

### 超调问题（类似 PID 调试）

如果你有控制理论背景，梯度下降和 PID 控制非常相似：

```
梯度下降              PID 控制
─────────────────────────────────
学习率                Kp（比例增益）
梯度                  误差
损失收敛              系统稳定
学习率太大 → 超调     Kp太大 → 超调
```

**学习率太大**：跨过谷底 → 来回震荡 → 甚至发散

**Adam 优化器**就像给梯度下降加了"积分"和"微分"项，让收敛更稳更快。

---

### 为什么到谷底会自动停？

```
谷底特点：导数 = 0（坡是平的）

更新公式：新权重 = 旧权重 - 学习率 × 0
                 = 旧权重（不动了）
```

梯度在最低点自动变成 0，所以不会"走过头"——除非学习率太大导致超调。

---

## 下一步学习

神经网络基础已经学完！接下来可以学习：

- **第5-7课**：卷积神经网络 (CNN) - 图像识别的利器
- **第8-10课**：人脸识别专题 - 理解你之前用的模型

或者直接动手：用 PyTorch 训练一个真正的模型！

---

<p align="center"><b>🎉 恭喜完成神经网络基础！你已经理解了深度学习的核心原理！</b></p>
