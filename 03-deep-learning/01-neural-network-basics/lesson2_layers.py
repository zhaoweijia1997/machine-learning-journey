"""
深度学习第2课：多层网络
=========================

上一课我们学了单个神经元，这一课来看看：
- 多个神经元如何组成"层"
- 多层如何组成"网络"
- 为什么需要多层？

还是用生活中的例子来理解！
"""

import numpy as np

# ====================
# 复习：单个神经元
# ====================

def sigmoid(x):
    """激活函数"""
    return 1 / (1 + np.exp(-x))

def neuron(inputs, weights, bias):
    """单个神经元的计算"""
    z = np.dot(inputs, weights) + bias
    return sigmoid(z)

print("=" * 50)
print("第2课：多层网络")
print("=" * 50)

# ====================
# 问题：判断一个水果好不好吃
# ====================

print("""
【场景】判断水果好不好吃

输入特征（0-1 之间）：
  - 甜度：越甜越好
  - 酸度：适中最好（太酸或不酸都不好）
  - 新鲜度：越新鲜越好

这个问题比"带不带伞"复杂！
- "酸度适中最好" 这种非线性关系，单个神经元很难处理
- 需要多个神经元协作！
""")

# ====================
# 方案：两层网络
# ====================

print("""
【解决方案】两层神经网络

输入层(3)     隐藏层(4)      输出层(1)
─────────    ──────────     ──────────
  甜度  ───┬──→ 神经元1 ──┬
           ├──→ 神经元2 ──┼──→ 好吃？
  酸度  ───┼──→ 神经元3 ──┤
           └──→ 神经元4 ──┘
  新鲜度 ──┘

- 输入层：3个特征（甜度、酸度、新鲜度）
- 隐藏层：4个神经元（学习不同的"判断标准"）
- 输出层：1个神经元（最终判断：好吃/不好吃）
""")

# ====================
# 代码实现
# ====================

print("-" * 50)
print("【代码实现】")
print("-" * 50)

# 隐藏层：4个神经元，每个有3个输入
# 权重矩阵：3输入 × 4神经元
weights_hidden = np.array([
    [0.8, -0.5, 0.3, 0.6],   # 甜度对4个神经元的权重
    [0.2, 0.9, -0.7, 0.1],   # 酸度对4个神经元的权重
    [0.5, 0.3, 0.8, 0.4],    # 新鲜度对4个神经元的权重
])
bias_hidden = np.array([-0.3, -0.4, 0.1, -0.2])

# 输出层：1个神经元，接收4个输入
weights_output = np.array([0.6, 0.4, 0.5, 0.3])
bias_output = -0.5

def forward(inputs):
    """
    前向传播：数据从输入流向输出
    """
    # 第1步：输入 → 隐藏层
    hidden = sigmoid(np.dot(inputs, weights_hidden) + bias_hidden)

    # 第2步：隐藏层 → 输出
    output = sigmoid(np.dot(hidden, weights_output) + bias_output)

    return hidden, output

print("""
权重矩阵的形状：
- 隐藏层权重: (3, 4) = 3个输入 × 4个神经元
- 输出层权重: (4,)  = 4个隐藏神经元 → 1个输出
""")

# ====================
# 测试不同的水果
# ====================

print("-" * 50)
print("【测试不同水果】")
print("-" * 50)

fruits = [
    ("完美草莓", [0.9, 0.3, 0.95]),  # 很甜、微酸、很新鲜
    ("酸柠檬",   [0.2, 0.95, 0.8]),  # 不甜、很酸、新鲜
    ("烂苹果",   [0.5, 0.4, 0.1]),   # 中等甜、中等酸、不新鲜
    ("甜橙子",   [0.85, 0.5, 0.7]),  # 很甜、适中酸、较新鲜
]

for name, features in fruits:
    hidden, output = forward(np.array(features))

    print(f"\n{name}:")
    print(f"  输入: 甜度={features[0]}, 酸度={features[1]}, 新鲜度={features[2]}")
    print(f"  隐藏层输出: {hidden.round(3)}")
    print(f"  最终得分: {output:.1%}", end="")

    if output > 0.7:
        print(" -> 好吃!")
    elif output > 0.4:
        print(" -> 一般般")
    else:
        print(" -> 不好吃")

# ====================
# 为什么需要隐藏层？
# ====================

print("\n" + "=" * 50)
print("【核心概念】为什么需要隐藏层？")
print("=" * 50)

print("""
单个神经元只能画"一条线"来分类：

    好吃
      │    /
      │   / ← 单个神经元只能画这种直线
      │  /
      │ /
    ──┴────── 不好吃

但现实问题往往是"曲线"的：
- "酸度适中最好" → 需要曲线
- "太甜也不好" → 需要曲线

多层网络可以画出复杂的"曲线"边界！

    好吃
      │   ╭──╮
      │  ╱    ╲  ← 多层网络可以画曲线
      │ ╱      ╲
      │╱        ╲
    ──┴────────── 不好吃
""")

# ====================
# 矩阵运算的直觉
# ====================

print("=" * 50)
print("【矩阵运算】为什么用矩阵？")
print("=" * 50)

print("""
假设隐藏层有4个神经元，每个都要计算：
  神经元1 = sigmoid(甜度×w1 + 酸度×w2 + 新鲜度×w3 + b1)
  神经元2 = sigmoid(甜度×w4 + 酸度×w5 + 新鲜度×w6 + b2)
  神经元3 = sigmoid(甜度×w7 + 酸度×w8 + 新鲜度×w9 + b3)
  神经元4 = sigmoid(甜度×w10 + 酸度×w11 + 新鲜度×w12 + b4)

写成矩阵形式（一行代码搞定）：
  hidden = sigmoid(inputs @ weights + bias)

矩阵运算的好处：
1. 代码简洁
2. GPU 可以并行加速（这就是为什么深度学习要用显卡！）
""")

# ====================
# 动手实验
# ====================

print("=" * 50)
print("【动手实验】")
print("=" * 50)

print("""
试试修改代码：
1. 改变水果的特征值，看输出怎么变
2. 增加隐藏层神经元数量（比如8个）
3. 添加第二个隐藏层

思考：
- 隐藏层越多越好吗？
- 神经元越多越好吗？
（提示：不是！过多会"过拟合"，后面会讲）
""")

# ====================
# 总结
# ====================

print("=" * 50)
print("【第2课总结】")
print("=" * 50)

print("""
[1] 学到的概念:
   - 层(Layer): 多个神经元并排工作
   - 隐藏层: 输入和输出之间的层, 学习"中间表示"
   - 前向传播: 数据从输入层流向输出层
   - 矩阵运算: 高效计算多个神经元

[2] 关键公式:
   hidden = sigmoid(inputs @ W1 + b1)
   output = sigmoid(hidden @ W2 + b2)

[3] 为什么需要多层:
   - 单层只能处理线性问题
   - 多层可以处理非线性复杂问题

[下一课预告] 第3课: 前向传播与损失函数
   - 网络输出和"正确答案"差多少?
   - 如何定义"好"与"不好"?
""")
